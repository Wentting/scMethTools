{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a647f810",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'type' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscMethtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuild_mtx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m import_bed_file\n",
      "File \u001b[1;32mD:\\Test\\scMethTools\\scMethtools\\preprocessing\\__init__.py:9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/python3\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m> Author: zongwt \u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m> Created Time: 2023年08月21日\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuild_mtx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mD:\\Test\\scMethTools\\scMethtools\\preprocessing\\build_mtx.py:98\u001b[0m\n\u001b[0;32m     94\u001b[0m         df_transposed\u001b[38;5;241m.\u001b[39mto_csv(stat_file, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stat_file\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_cov_to_coo\u001b[39m(\n\u001b[1;32m---> 98\u001b[0m         out_dir: \u001b[43mPath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     99\u001b[0m         cov_file: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    100\u001b[0m         chunksize : \u001b[38;5;28mint\u001b[39m  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m,\n\u001b[0;32m    101\u001b[0m         cell_id: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    102\u001b[0m         out_file:\u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    103\u001b[0m         round_sites : \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    104\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    105\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    convert single methylation cov/bed file into coo_matrix with fixed batch in temp dir\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    :param out_dir:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     cov_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(cov_file, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    116\u001b[0m                          names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrand\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmc_class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmc_context\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmc_level\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethylated\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    117\u001b[0m                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# usecols=['chr','pos','mc_level','total']\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'type' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "from scMethtools import preprocessing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28ad0bc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scMethtools.preprocessing' has no attribute 'import_bed_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_bed_file\u001b[49m(\n\u001b[0;32m      2\u001b[0m     data_dir \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mTest/GSE56789/raw/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mTest/GSE56789/temp/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'scMethtools.preprocessing' has no attribute 'import_bed_file'"
     ]
    }
   ],
   "source": [
    "pp.import_bed_file(\n",
    "    data_dir ='D:\\\\Test/GSE56789/raw/',\n",
    "    output_dir = 'D:\\\\Test/GSE56789/temp/',\n",
    ")\n",
    "pp.convert_coo_file_to_matrix('D:\\\\Test/GSE56789/temp','chr1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3091aafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.17\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35ab8b16-c4b0-4e22-b856-4b921e9daa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import ridge_regression\n",
    "\n",
    "def inv_logit_mat(x, min=0.0, max=1.0,):\n",
    "    # the inverse logit transformation function\n",
    "    p = np.exp(x) / (1.0+np.exp(x))\n",
    "    which_large = np.isnan(p) & (~np.isnan(x))\n",
    "    p[which_large] = 1.0\n",
    "    return p*(max-min)+min\n",
    "\n",
    "def sparse_logistic_pca(\n",
    "        dat, lbd=0.0006, k=2, verbose=False, max_iters=100, crit=1e-5,\n",
    "        randstart=False, procrustes=True, lasso=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    A Python implementation of the sparse logistic PCA of the following paper:\n",
    "        Lee, S., Huang, J. Z., & Hu, J. (2010). Sparse logistic principal components analysis for binary data.\n",
    "        The annals of applied statistics, 4(3), 1579.\n",
    "\n",
    "    This implementation is migrated from this R package:\n",
    "        https://github.com/andland/SparseLogisticPCA\n",
    "\n",
    "    Args:\n",
    "        dat: input data, n*d numpy array where n is the numbers of samples and d is the feature dimensionality\n",
    "        lbd: the lambda value, higher value will lead to more sparse components\n",
    "        k: the dimension after reduction\n",
    "        verbose: print log or not\n",
    "        max_iters: maximum number of iterations\n",
    "        crit: the minimum difference criteria for stopping training\n",
    "        randstart: randomly initialize A, B, mu or not\n",
    "        procrustes: procrustes\n",
    "        lasso: whether to use LASSO solver\n",
    "\n",
    "    Returns: a dict containing the results\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### Initialize q\n",
    "    q = 2*dat-1\n",
    "    q[np.isnan(q)] = 0.0\n",
    "    n,d = dat.shape\n",
    "\n",
    "    ### Initialize mu, A, B\n",
    "    if not randstart:\n",
    "        mu = np.mean(q,axis=0)\n",
    "        udv_u, udv_d, udv_v = np.linalg.svd(q-np.mean(q,axis=0), full_matrices=False)\n",
    "        A = udv_u[:,0:k].copy()\n",
    "        B = np.matmul(udv_v[0:k,:].T, np.diag(udv_d[0:k]))\n",
    "    else:\n",
    "        mu = np.random.normal(size=(d,))\n",
    "        A = np.random.uniform(low=-1.0, high=1.0, size=(n,k,))\n",
    "        B = np.random.uniform(low=-1.0, high=1.0, size=(d,k,))\n",
    "\n",
    "    loss_trace = dict()\n",
    "\n",
    "    ## loop to optimize the loss, see Alogrithm 1 in the paper\n",
    "    for m in range(max_iters):\n",
    "\n",
    "        last_mu, last_A, last_B = mu.copy(), A.copy(), B.copy()\n",
    "\n",
    "        theta = np.outer(np.ones(n), mu) + np.matmul(A, B.T)\n",
    "        X = theta+4*q*(1-inv_logit_mat(q*theta))\n",
    "        Xcross = X - np.matmul(A, B.T)\n",
    "        mu = np.matmul((1.0/n) * Xcross.T, np.ones(n))\n",
    "\n",
    "        theta = np.outer(np.ones(n), mu) + np.matmul(A, B.T)\n",
    "        X = theta+4*q*(1-inv_logit_mat(q*theta))\n",
    "        Xstar = X-np.outer(np.ones(n), mu)\n",
    "\n",
    "        if procrustes:\n",
    "            M_u, M_d, M_v = np.linalg.svd(np.matmul(Xstar, B), full_matrices=False)\n",
    "            A = np.matmul(M_u, M_v)\n",
    "        else:\n",
    "            A = Xstar @ B @ np.linalg.inv(B.T @ B)\n",
    "            A, _ = np.linalg.qr(A)\n",
    "\n",
    "        theta = np.outer(np.ones(n), mu) + A @ B.T\n",
    "        X = theta + 4 * q * (1 - inv_logit_mat(q * theta))\n",
    "        Xstar = X-np.outer(np.ones(n), mu)\n",
    "\n",
    "        if lasso:\n",
    "            B_lse = Xstar.T @ A\n",
    "            B = np.sign(B_lse) * np.maximum(0.0, np.abs(B_lse)-4*n*lbd)\n",
    "        else:\n",
    "            C = Xstar.T @ A\n",
    "            B = (np.abs(B) / (np.abs(B)+4*n*lbd)) * C\n",
    "\n",
    "        q_dot_theta = q*(np.outer(np.ones(n),mu) + A @ B.T)\n",
    "        loglike = np.sum(np.log(inv_logit_mat(q_dot_theta))[~np.isnan(dat)])\n",
    "        penalty = n*lbd*np.sum(abs(B))\n",
    "        loss_trace[str(m)] = (-loglike+penalty) / np.sum(~np.isnan(dat))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Iter: {m} - Loss: {loss_trace[str(m)]:.4f}, NegLogLike: {-loglike:.4f}, Penalty: {penalty:.4f} \")\n",
    "\n",
    "        if m>3:\n",
    "            if loss_trace[str(m-1)] - loss_trace[str(m)] < crit:\n",
    "                break\n",
    "\n",
    "    if loss_trace[str(m-1)] < loss_trace[str(m)]:\n",
    "        mu, A, B, m = last_mu, last_A, last_B, m-1\n",
    "\n",
    "        q_dot_theta = q*(np.outer(np.ones(n),mu) + A @ B.T)\n",
    "        loglike = np.sum(np.log(inv_logit_mat(q_dot_theta))[~np.isnan(dat)])\n",
    "\n",
    "    zeros = sum(np.abs(B))\n",
    "    BIC = -2.0*loglike+np.log(n)*(d+n*k+np.sum(np.abs(B)>=1e-10))\n",
    "\n",
    "    res = {\n",
    "        \"mu\":mu, \"A\":A, \"B\":B, \"zeros\":zeros,\n",
    "        \"BIC\":BIC, \"iters\":m, \"loss_trace\":loss_trace, \"lambda\":lbd,\n",
    "    }\n",
    "\n",
    "    return res\n",
    "\n",
    "class SparseLogisticPCA(object):\n",
    "    \"\"\"\n",
    "    A warper class of sparse logistic PCA, which provides the fit, transform and fit_transform methods\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, lbd=0.0001, n_components=2, verbose=False, max_iters=100, crit=1e-5,\n",
    "            randstart=False, procrustes=True, lasso=True,\n",
    "            ridge_alpha=0.01,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lbd: the lambda value, higher value will lead to more sparse components\n",
    "            n_components: the dimension after reduction, i.e. k in the origin paper\n",
    "            verbose: print log or not\n",
    "            max_iters: maximum number of iterations\n",
    "            crit: the minimum difference criteria for stopping training\n",
    "            randstart: randomly initialize A, B, mu or not\n",
    "            procrustes: procrustes\n",
    "            lasso: whether to use LASSO solver\n",
    "            ridge_alpha: Amount of ridge shrinkage to apply in order to improve conditioning when\n",
    "                calling the transform method.\n",
    "        \"\"\"\n",
    "        self.lbd = lbd\n",
    "        self.n_components = n_components\n",
    "        self.verbose=verbose\n",
    "        self.max_iters = max_iters\n",
    "        self.crit = crit\n",
    "        self.randstart = randstart\n",
    "        self.procrustes = procrustes\n",
    "        self.lasso=lasso\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "\n",
    "    def fit(self, dat, verbose=False):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            dat: ndarray of shape (n_samples, n_features), the data to be fitted\n",
    "            verbose: print log or not\n",
    "\n",
    "        Returns:\n",
    "            self\n",
    "\n",
    "        \"\"\"\n",
    "        res = sparse_logistic_pca(\n",
    "            dat, lbd=self.lbd, k=self.n_components, verbose=verbose,\n",
    "            max_iters=self.max_iters, crit=self.crit,\n",
    "            randstart=self.randstart, procrustes=self.procrustes,\n",
    "            lasso=self.lasso,)\n",
    "\n",
    "        self.mu, self.components_ = res['mu'], res['B'].T\n",
    "        _, self.d = dat.shape\n",
    "\n",
    "        components_norm = np.linalg.norm(self.components_, axis=1)[:, np.newaxis]\n",
    "        components_norm[components_norm == 0] = 1\n",
    "        self.components_ /= components_norm\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "\n",
    "        Similar to Sparse PCA, the orthogonality of the learned components is not enforced in Sparse Logistic PCA,\n",
    "            and hence one cannot use a simple linear projection.\n",
    "\n",
    "        The origin paper does not describe how to transform the new data, and this implementation of transform\n",
    "            function generally follows that of sklearn.decomposition.SparsePCA:\n",
    "            https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html\n",
    "\n",
    "        The handling of missing data (N/A) is not supported in this transform implementation.\n",
    "\n",
    "        Args:\n",
    "            X: ndarray of shape (n_samples, n_features), the input data\n",
    "\n",
    "        Returns:\n",
    "            ndarray of (n_samples, n_components), the data after dimensionality reduction\n",
    "\n",
    "        \"\"\"\n",
    "        n, d = X.shape\n",
    "        assert d==self.d,\\\n",
    "            f\"Input data should have a shape (n_samples, n_features) and n_features should be {self.d}\"\n",
    "\n",
    "        Xstar = X - np.outer(np.ones(n), self.mu)\n",
    "\n",
    "        U = ridge_regression(\n",
    "            self.components_.T, Xstar.T, self.ridge_alpha, solver=\"cholesky\",\n",
    "        )\n",
    "\n",
    "        return U\n",
    "\n",
    "    def fit_transform(self, dat):\n",
    "        \"\"\"\n",
    "        Fit the model with X and apply the dimensionality reduction on X.\n",
    "\n",
    "        Args:\n",
    "            dat: ndarray of shape (n_samples, n_features)\n",
    "\n",
    "        Returns:\n",
    "            ndarray of (n_samples, n_components)\n",
    "\n",
    "        \"\"\"\n",
    "        self.fit(dat)\n",
    "        return self.transform(dat)\n",
    "\n",
    "    def fine_tune_lambdas(self, dat, lambdas=np.arange(0, 0.00061, 0.0006 / 10)):\n",
    "        # fine tune the Lambda values based on BICs following the paper\n",
    "        BICs, zeros = [], []\n",
    "        for lbd in lambdas:\n",
    "            # print(f\"Lambda: {lbd:.6f}\")\n",
    "            this_res = sparse_logistic_pca(\n",
    "            dat, lbd=lbd, k=self.n_components, verbose=False,\n",
    "            max_iters=self.max_iters, crit=self.crit,\n",
    "            randstart=self.randstart, procrustes=self.procrustes,\n",
    "            lasso=self.lasso,)\n",
    "            BICs.append(this_res['BIC'])\n",
    "            zeros.append(this_res['zeros'])\n",
    "        best_ldb = lambdas[np.argmin(BICs)]\n",
    "        return best_ldb, BICs, zeros\n",
    "\n",
    "    def set_lambda(self,new_lbd):\n",
    "        print(f\"Setting lambda to: {new_lbd}\")\n",
    "        self.lbd = new_lbd\n",
    "\n",
    "    def set_ridge_alpha(self, ridge_alpha):\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "\n",
    "    def set_n_components(self, n_components):\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def get_components(self):\n",
    "        return self.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac9b144-f256-4113-8114-9670f0480cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = np.random.randint(2, size=(40,16,),).astype(float)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd9f0e0-c84c-402f-95b5-2e3c81fe6df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.],\n",
       "       [0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.],\n",
       "       [1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.],\n",
       "       [1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.],\n",
       "       [1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
       "       [0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.],\n",
       "       [0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.],\n",
       "       [0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1.],\n",
       "       [1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.],\n",
       "       [1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.],\n",
       "       [1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.],\n",
       "       [1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.],\n",
       "       [0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.],\n",
       "       [1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1.],\n",
       "       [1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e257ec-8424-4fcf-8823-b01e61975d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 - Loss: 0.5289, NegLogLike: 337.9820, Penalty: 0.5360 \n",
      "Iter: 1 - Loss: 0.5213, NegLogLike: 333.0097, Penalty: 0.6203 \n",
      "Iter: 2 - Loss: 0.5177, NegLogLike: 330.6690, Penalty: 0.6670 \n",
      "Iter: 3 - Loss: 0.5153, NegLogLike: 329.0852, Penalty: 0.6990 \n",
      "Iter: 4 - Loss: 0.5134, NegLogLike: 327.8378, Penalty: 0.7238 \n",
      "Iter: 5 - Loss: 0.5118, NegLogLike: 326.7885, Penalty: 0.7447 \n",
      "Iter: 6 - Loss: 0.5104, NegLogLike: 325.8790, Penalty: 0.7634 \n",
      "Iter: 7 - Loss: 0.5092, NegLogLike: 325.0789, Penalty: 0.7812 \n",
      "Iter: 8 - Loss: 0.5081, NegLogLike: 324.3695, Penalty: 0.7980 \n",
      "Iter: 9 - Loss: 0.5071, NegLogLike: 323.7361, Penalty: 0.8140 \n",
      "Iter: 10 - Loss: 0.5062, NegLogLike: 323.1679, Penalty: 0.8293 \n",
      "Iter: 11 - Loss: 0.5055, NegLogLike: 322.6555, Penalty: 0.8440 \n",
      "Iter: 12 - Loss: 0.5048, NegLogLike: 322.1912, Penalty: 0.8581 \n",
      "Iter: 13 - Loss: 0.5041, NegLogLike: 321.7682, Penalty: 0.8718 \n",
      "Iter: 14 - Loss: 0.5035, NegLogLike: 321.3807, Penalty: 0.8851 \n",
      "Iter: 15 - Loss: 0.5030, NegLogLike: 321.0239, Penalty: 0.8979 \n",
      "Iter: 16 - Loss: 0.5025, NegLogLike: 320.6936, Penalty: 0.9105 \n",
      "Iter: 17 - Loss: 0.5020, NegLogLike: 320.3867, Penalty: 0.9228 \n",
      "Iter: 18 - Loss: 0.5016, NegLogLike: 320.0999, Penalty: 0.9348 \n",
      "Iter: 19 - Loss: 0.5012, NegLogLike: 319.8307, Penalty: 0.9464 \n",
      "Iter: 20 - Loss: 0.5008, NegLogLike: 319.5771, Penalty: 0.9577 \n",
      "Iter: 21 - Loss: 0.5005, NegLogLike: 319.3373, Penalty: 0.9687 \n",
      "Iter: 22 - Loss: 0.5001, NegLogLike: 319.1097, Penalty: 0.9794 \n",
      "Iter: 23 - Loss: 0.4998, NegLogLike: 318.8932, Penalty: 0.9898 \n",
      "Iter: 24 - Loss: 0.4995, NegLogLike: 318.6866, Penalty: 0.9999 \n",
      "Iter: 25 - Loss: 0.4992, NegLogLike: 318.4888, Penalty: 1.0098 \n",
      "Iter: 26 - Loss: 0.4989, NegLogLike: 318.2992, Penalty: 1.0195 \n",
      "Iter: 27 - Loss: 0.4987, NegLogLike: 318.1169, Penalty: 1.0289 \n",
      "Iter: 28 - Loss: 0.4984, NegLogLike: 317.9413, Penalty: 1.0381 \n",
      "Iter: 29 - Loss: 0.4982, NegLogLike: 317.7719, Penalty: 1.0472 \n",
      "Iter: 30 - Loss: 0.4979, NegLogLike: 317.6080, Penalty: 1.0560 \n",
      "Iter: 31 - Loss: 0.4977, NegLogLike: 317.4494, Penalty: 1.0646 \n",
      "Iter: 32 - Loss: 0.4975, NegLogLike: 317.2955, Penalty: 1.0731 \n",
      "Iter: 33 - Loss: 0.4972, NegLogLike: 317.1459, Penalty: 1.0814 \n",
      "Iter: 34 - Loss: 0.4970, NegLogLike: 317.0005, Penalty: 1.0896 \n",
      "Iter: 35 - Loss: 0.4968, NegLogLike: 316.8587, Penalty: 1.0976 \n",
      "Iter: 36 - Loss: 0.4966, NegLogLike: 316.7205, Penalty: 1.1055 \n",
      "Iter: 37 - Loss: 0.4964, NegLogLike: 316.5855, Penalty: 1.1132 \n",
      "Iter: 38 - Loss: 0.4962, NegLogLike: 316.4534, Penalty: 1.1208 \n",
      "Iter: 39 - Loss: 0.4960, NegLogLike: 316.3242, Penalty: 1.1282 \n",
      "Iter: 40 - Loss: 0.4958, NegLogLike: 316.1975, Penalty: 1.1356 \n",
      "Iter: 41 - Loss: 0.4956, NegLogLike: 316.0732, Penalty: 1.1428 \n",
      "Iter: 42 - Loss: 0.4955, NegLogLike: 315.9511, Penalty: 1.1499 \n",
      "Iter: 43 - Loss: 0.4953, NegLogLike: 315.8311, Penalty: 1.1569 \n",
      "Iter: 44 - Loss: 0.4951, NegLogLike: 315.7130, Penalty: 1.1637 \n",
      "Iter: 45 - Loss: 0.4949, NegLogLike: 315.5966, Penalty: 1.1705 \n",
      "Iter: 46 - Loss: 0.4948, NegLogLike: 315.4818, Penalty: 1.1771 \n",
      "Iter: 47 - Loss: 0.4946, NegLogLike: 315.3686, Penalty: 1.1837 \n",
      "Iter: 48 - Loss: 0.4944, NegLogLike: 315.2567, Penalty: 1.1901 \n",
      "Iter: 49 - Loss: 0.4943, NegLogLike: 315.1461, Penalty: 1.1965 \n",
      "Iter: 50 - Loss: 0.4941, NegLogLike: 315.0367, Penalty: 1.2027 \n",
      "Iter: 51 - Loss: 0.4940, NegLogLike: 314.9282, Penalty: 1.2089 \n",
      "Iter: 52 - Loss: 0.4938, NegLogLike: 314.8208, Penalty: 1.2149 \n",
      "Iter: 53 - Loss: 0.4936, NegLogLike: 314.7141, Penalty: 1.2209 \n",
      "Iter: 54 - Loss: 0.4935, NegLogLike: 314.6083, Penalty: 1.2268 \n",
      "Iter: 55 - Loss: 0.4933, NegLogLike: 314.5031, Penalty: 1.2326 \n",
      "Iter: 56 - Loss: 0.4932, NegLogLike: 314.3985, Penalty: 1.2383 \n",
      "Iter: 57 - Loss: 0.4930, NegLogLike: 314.2944, Penalty: 1.2439 \n",
      "Iter: 58 - Loss: 0.4929, NegLogLike: 314.1907, Penalty: 1.2494 \n",
      "Iter: 59 - Loss: 0.4927, NegLogLike: 314.0874, Penalty: 1.2549 \n",
      "Iter: 60 - Loss: 0.4926, NegLogLike: 313.9845, Penalty: 1.2602 \n",
      "Iter: 61 - Loss: 0.4924, NegLogLike: 313.8817, Penalty: 1.2655 \n",
      "Iter: 62 - Loss: 0.4923, NegLogLike: 313.7791, Penalty: 1.2707 \n",
      "Iter: 63 - Loss: 0.4921, NegLogLike: 313.6767, Penalty: 1.2759 \n",
      "Iter: 64 - Loss: 0.4920, NegLogLike: 313.5743, Penalty: 1.2809 \n",
      "Iter: 65 - Loss: 0.4918, NegLogLike: 313.4719, Penalty: 1.2859 \n",
      "Iter: 66 - Loss: 0.4917, NegLogLike: 313.3695, Penalty: 1.2908 \n",
      "Iter: 67 - Loss: 0.4915, NegLogLike: 313.2671, Penalty: 1.2956 \n",
      "Iter: 68 - Loss: 0.4914, NegLogLike: 313.1646, Penalty: 1.3004 \n",
      "Iter: 69 - Loss: 0.4912, NegLogLike: 313.0620, Penalty: 1.3051 \n",
      "Iter: 70 - Loss: 0.4910, NegLogLike: 312.9593, Penalty: 1.3097 \n",
      "Iter: 71 - Loss: 0.4909, NegLogLike: 312.8564, Penalty: 1.3142 \n",
      "Iter: 72 - Loss: 0.4907, NegLogLike: 312.7534, Penalty: 1.3187 \n",
      "Iter: 73 - Loss: 0.4906, NegLogLike: 312.6503, Penalty: 1.3231 \n",
      "Iter: 74 - Loss: 0.4904, NegLogLike: 312.5470, Penalty: 1.3275 \n",
      "Iter: 75 - Loss: 0.4903, NegLogLike: 312.4437, Penalty: 1.3318 \n",
      "Iter: 76 - Loss: 0.4901, NegLogLike: 312.3402, Penalty: 1.3360 \n",
      "Iter: 77 - Loss: 0.4900, NegLogLike: 312.2367, Penalty: 1.3402 \n",
      "Iter: 78 - Loss: 0.4898, NegLogLike: 312.1332, Penalty: 1.3443 \n",
      "Iter: 79 - Loss: 0.4897, NegLogLike: 312.0297, Penalty: 1.3484 \n",
      "Iter: 80 - Loss: 0.4895, NegLogLike: 311.9263, Penalty: 1.3524 \n",
      "Iter: 81 - Loss: 0.4893, NegLogLike: 311.8231, Penalty: 1.3563 \n",
      "Iter: 82 - Loss: 0.4892, NegLogLike: 311.7200, Penalty: 1.3603 \n",
      "Iter: 83 - Loss: 0.4890, NegLogLike: 311.6173, Penalty: 1.3641 \n",
      "Iter: 84 - Loss: 0.4889, NegLogLike: 311.5149, Penalty: 1.3680 \n",
      "Iter: 85 - Loss: 0.4887, NegLogLike: 311.4130, Penalty: 1.3718 \n",
      "Iter: 86 - Loss: 0.4886, NegLogLike: 311.3116, Penalty: 1.3755 \n",
      "Iter: 87 - Loss: 0.4884, NegLogLike: 311.2108, Penalty: 1.3792 \n",
      "Iter: 88 - Loss: 0.4883, NegLogLike: 311.1107, Penalty: 1.3829 \n",
      "Iter: 89 - Loss: 0.4881, NegLogLike: 311.0115, Penalty: 1.3866 \n",
      "Iter: 90 - Loss: 0.4880, NegLogLike: 310.9132, Penalty: 1.3903 \n",
      "Iter: 91 - Loss: 0.4878, NegLogLike: 310.8158, Penalty: 1.3939 \n",
      "Iter: 92 - Loss: 0.4877, NegLogLike: 310.7195, Penalty: 1.3975 \n",
      "Iter: 93 - Loss: 0.4875, NegLogLike: 310.6244, Penalty: 1.4011 \n",
      "Iter: 94 - Loss: 0.4874, NegLogLike: 310.5304, Penalty: 1.4047 \n",
      "Iter: 95 - Loss: 0.4873, NegLogLike: 310.4378, Penalty: 1.4083 \n",
      "Iter: 96 - Loss: 0.4871, NegLogLike: 310.3465, Penalty: 1.4118 \n",
      "Iter: 97 - Loss: 0.4870, NegLogLike: 310.2567, Penalty: 1.4154 \n",
      "Iter: 98 - Loss: 0.4869, NegLogLike: 310.1683, Penalty: 1.4190 \n",
      "Iter: 99 - Loss: 0.4867, NegLogLike: 310.0813, Penalty: 1.4225 \n"
     ]
    }
   ],
   "source": [
    "# the binary matrix to fit, shape: (n_samples, n_features)\n",
    "dat = np.random.randint(2, size=(40,16,),).astype(float)  \n",
    "\n",
    "# create model and fit data\n",
    "# lbd: lambda controlling sparsity of components\n",
    "SLPCA = SparseLogisticPCA(n_components=2, lbd=0.0001)  \n",
    "SLPCA.fit(dat, verbose=True)  \n",
    "\n",
    "# the binary matrix to transform, shape: (n_samples, n_features)\n",
    "X = np.random.randint(2, size=(20,16,),).astype(float)  \n",
    "\n",
    "# get the transformed data\n",
    "X_t = SLPCA.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1badb7b0-9c7c-470e-9005-13cbb4ea16e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.79515761, -0.75928224],\n",
       "       [-0.8479762 , -1.08003798],\n",
       "       [-0.60091912, -1.24437601],\n",
       "       [-1.21029583,  0.13036661],\n",
       "       [-0.71705681, -1.05627988],\n",
       "       [-0.26945753, -0.62709579],\n",
       "       [ 0.0625411 ,  0.14218722],\n",
       "       [-1.5196023 , -0.35777352],\n",
       "       [-0.39867368, -0.85970238],\n",
       "       [-1.05767814, -0.15210979],\n",
       "       [ 0.24221953, -0.4310016 ],\n",
       "       [-0.18951689,  0.08387088],\n",
       "       [-1.40656054, -0.11462958],\n",
       "       [-0.31584435, -0.93718864],\n",
       "       [ 0.05886169, -0.51000958],\n",
       "       [-0.35764222, -0.22365524],\n",
       "       [-1.12846303,  0.49009269],\n",
       "       [-0.982654  , -0.18781202],\n",
       "       [-0.21662577, -0.07776975],\n",
       "       [-0.87780215,  0.00168608]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe9b246-085d-45b5-acb9-485340b92fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23191565, -0.02982951,  0.06358042, -0.10853536, -0.05287583,\n",
       "         0.71656663,  0.41307586,  0.02726139,  0.1629159 , -0.43555671,\n",
       "         0.09530327,  0.05897715,  0.05660401, -0.07334414,  0.0409295 ,\n",
       "        -0.05285486],\n",
       "       [-0.29585076,  0.05299171,  0.08320947, -0.0478146 , -0.01170234,\n",
       "         0.51627725, -0.58702832, -0.20643897, -0.11959264,  0.23506211,\n",
       "         0.02503766, -0.00091993,  0.02076631,  0.41781461,  0.03276973,\n",
       "        -0.01724099]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components = SLPCA.get_components()\n",
    "components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4510e9ff-8ffb-4f15-861f-6fb9b2921ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad \n",
    "cdata = ad.read('D://Test/scMeth/enhancer_c1_CG_paper.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61cf635f-d585-4950-9781-5fee5b936702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3379, 55017)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdata.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430b1d6e-5d14-407c-8042-7e9d782ad383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and fit data\n",
    "# lbd: lambda controlling sparsity of components\n",
    "SLPCA = SparseLogisticPCA(n_components=2, lbd=0.0001)  \n",
    "SLPCA.fit(cdata.X, verbose=True)  \n",
    "\n",
    "# the binary matrix to transform, shape: (n_samples, n_features)\n",
    "X = np.random.randint(2, size=(20,16,),).astype(float)  \n",
    "\n",
    "# get the transformed data\n",
    "X_t = SLPCA.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99fcee1a-fdb4-43f1-93e6-f0f3eeffe9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d25eb385-14aa-41ef-ac98-6725d072853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,\n",
    "                                                  n_iter=50, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79238f09-39e2-411a-8ee7-ad7b0b33e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import SparsePCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48194531-4d2b-4d1c-a53d-b322c9dfb615",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSparsePCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m sparse_pca \u001b[38;5;241m=\u001b[39m SparsePCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)  \u001b[38;5;66;03m# 你可以根据需要选择合适的超参数\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 拟合Sparse PCA模型\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43msparse_pca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 获取稀疏主成分和它们的权重\u001b[39;00m\n\u001b[0;32m      8\u001b[0m sparse_components \u001b[38;5;241m=\u001b[39m sparse_pca\u001b[38;5;241m.\u001b[39mcomponents_\n",
      "File \u001b[1;32mD:\\conda\\envs\\py39\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\conda\\envs\\py39\\lib\\site-packages\\sklearn\\decomposition\\_sparse_pca.py:80\u001b[0m, in \u001b[0;36m_BaseSparsePCA.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model from data in X.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    Returns the instance itself.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     79\u001b[0m random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[1;32m---> 80\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     83\u001b[0m X \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_\n",
      "File \u001b[1;32mD:\\conda\\envs\\py39\\lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mD:\\conda\\envs\\py39\\lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mD:\\conda\\envs\\py39\\lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\conda\\envs\\py39\\lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nSparsePCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# 创建Sparse PCA模型\n",
    "sparse_pca = SparsePCA(n_components=10, alpha=0.1)  # 你可以根据需要选择合适的超参数\n",
    "\n",
    "# 拟合Sparse PCA模型\n",
    "sparse_pca.fit(cdata.X)\n",
    "\n",
    "# 获取稀疏主成分和它们的权重\n",
    "sparse_components = sparse_pca.components_\n",
    "\n",
    "# 降维\n",
    "X_transformed = sparse_pca.transform(cdata.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d44f43ce-69e3-4367-9872-09baa28335cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.849964  ,  -0.17153347,   0.8072775 , ...,  -1.89278527,\n",
       "         -1.83773287,  -2.6370121 ],\n",
       "       [  0.95847795,  -1.02198946,   0.7036406 , ...,   0.35725168,\n",
       "          0.48607854,  -2.22351473],\n",
       "       [ -2.70793834,  -4.00148546,  -1.72552943, ...,  -5.21196656,\n",
       "         -6.34598292,  -9.52472873],\n",
       "       ...,\n",
       "       [-52.45279073, -40.56265838, -27.9396152 , ...,  -4.54400283,\n",
       "        -22.98919745, -11.89009458],\n",
       "       [-57.42504199, -46.91738255, -33.60712335, ...,  -8.83782935,\n",
       "        -29.85530106, -15.58977452],\n",
       "       [-59.68001976, -47.61288098, -33.3117344 , ..., -10.87402717,\n",
       "        -30.72842098, -15.23069623]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import\n",
    "from scipy.linalg import toeplitz\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "\n",
    "# simulate data\n",
    "K = 10 - toeplitz(np.arange(10))\n",
    "data1 = np.cumsum(np.random.multivariate_normal(np.zeros(10), K, 250), axis=0)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86497448-351d-4157-aab3-068b3b58d395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hypertools\n",
      "  Downloading hypertools-0.8.0-py3-none-any.whl (59 kB)\n",
      "     ---------------------------------------- 0.0/59.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/59.7 kB ? eta -:--:--\n",
      "     -------------------- ------------------- 30.7/59.7 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 59.7/59.7 kB 797.7 kB/s eta 0:00:00\n",
      "Collecting PPCA>=0.0.2 (from hypertools)\n",
      "  Downloading ppca-0.0.4-py3-none-any.whl (6.7 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in d:\\conda\\envs\\py39\\lib\\site-packages (from hypertools) (1.3.0)\n",
      "Requirement already satisfied: pandas>=0.18.0 in d:\\conda\\envs\\py39\\lib\\site-packages (from hypertools) (2.0.3)\n",
      "Requirement already satisfied: seaborn>=0.8.1 in d:\\conda\\envs\\py39\\lib\\site-packages (from hypertools) (0.12.2)\n",
      "Requirement already satisfied: matplotlib>=1.5.1 in d:\\conda\\envs\\py39\\lib\\site-packages (from hypertools) (3.7.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in d:\\conda\\envs\\py39\\lib\\site-packages (from hypertools) (1.11.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in d:\\conda\\envs\\py39\\lib\\site-packages (from hypertools) (1.24.4)\n",
      "Requirement already satisfied: umap-learn>=0.4.6 in d:\\conda\\envs\\py39\\lib\\site-packages (from hypertools) (0.5.3)\n",
      "Requirement already satisfied: requests in d:\\conda\\envs\\py39\\lib\\site-packages (from hypertools) (2.31.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\conda\\envs\\py39\\lib\\site-packages (from matplotlib>=1.5.1->hypertools) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\conda\\envs\\py39\\lib\\site-packages (from matplotlib>=1.5.1->hypertools) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\conda\\envs\\py39\\lib\\site-packages (from matplotlib>=1.5.1->hypertools) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\conda\\envs\\py39\\lib\\site-packages (from matplotlib>=1.5.1->hypertools) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\conda\\envs\\py39\\lib\\site-packages (from matplotlib>=1.5.1->hypertools) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\conda\\envs\\py39\\lib\\site-packages (from matplotlib>=1.5.1->hypertools) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in d:\\conda\\envs\\py39\\lib\\site-packages (from matplotlib>=1.5.1->hypertools) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\conda\\envs\\py39\\lib\\site-packages (from matplotlib>=1.5.1->hypertools) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in d:\\conda\\envs\\py39\\lib\\site-packages (from matplotlib>=1.5.1->hypertools) (6.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\conda\\envs\\py39\\lib\\site-packages (from pandas>=0.18.0->hypertools) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\conda\\envs\\py39\\lib\\site-packages (from pandas>=0.18.0->hypertools) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\conda\\envs\\py39\\lib\\site-packages (from scikit-learn>=0.24->hypertools) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\conda\\envs\\py39\\lib\\site-packages (from scikit-learn>=0.24->hypertools) (3.2.0)\n",
      "Requirement already satisfied: numba>=0.49 in d:\\conda\\envs\\py39\\lib\\site-packages (from umap-learn>=0.4.6->hypertools) (0.57.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in d:\\conda\\envs\\py39\\lib\\site-packages (from umap-learn>=0.4.6->hypertools) (0.5.10)\n",
      "Requirement already satisfied: tqdm in d:\\conda\\envs\\py39\\lib\\site-packages (from umap-learn>=0.4.6->hypertools) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\conda\\envs\\py39\\lib\\site-packages (from requests->hypertools) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\conda\\envs\\py39\\lib\\site-packages (from requests->hypertools) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\conda\\envs\\py39\\lib\\site-packages (from requests->hypertools) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\conda\\envs\\py39\\lib\\site-packages (from requests->hypertools) (2023.7.22)\n",
      "Requirement already satisfied: zipp>=3.1.0 in d:\\conda\\envs\\py39\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=1.5.1->hypertools) (3.16.2)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in d:\\conda\\envs\\py39\\lib\\site-packages (from numba>=0.49->umap-learn>=0.4.6->hypertools) (0.40.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\conda\\envs\\py39\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=1.5.1->hypertools) (1.16.0)\n",
      "Requirement already satisfied: colorama in d:\\conda\\envs\\py39\\lib\\site-packages (from tqdm->umap-learn>=0.4.6->hypertools) (0.4.6)\n",
      "Installing collected packages: PPCA, hypertools\n",
      "Successfully installed PPCA-0.0.4 hypertools-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install hypertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17650577-27c2-4d34-bdc0-b6b758f1dedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\py39\\lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "D:\\conda\\envs\\py39\\lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "D:\\conda\\envs\\py39\\lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "D:\\conda\\envs\\py39\\lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "D:\\conda\\envs\\py39\\lib\\site-packages\\hypertools\\tools\\format_data.py:140: UserWarning: Missing data: Inexact solution computed with PPCA (see https://github.com/allentran/pca-magic for details)\n",
      "  warnings.warn('Missing data: Inexact solution computed with PPCA (see https://github.com/allentran/pca-magic for details)')\n",
      "D:\\conda\\envs\\py39\\lib\\site-packages\\hypertools\\plot\\draw.py:70: UserWarning: linestyle is redundantly defined by the 'linestyle' keyword argument and the fmt string \"-\" (-> linestyle='-'). The keyword argument will take precedence.\n",
      "  ax.plot(data[i][:,0], data[i][:,1], data[i][:,2], fmt[i], **ikwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hypertools.datageometry.DataGeometry at 0x239089fa490>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import\n",
    "from scipy.linalg import toeplitz\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "import hypertools as hyp\n",
    "\n",
    "# simulate data\n",
    "K = 10 - toeplitz(np.arange(10))\n",
    "data1 = np.cumsum(np.random.multivariate_normal(np.zeros(10), K, 250), axis=0)\n",
    "data2 = copy(data1)\n",
    "\n",
    "# simulate missing data\n",
    "missing = .1\n",
    "inds = [(i,j) for i in range(data2.shape[0]) for j in range(data2.shape[1])]\n",
    "missing_data = [inds[i] for i in np.random.choice(int(len(inds)), int(len(inds)*missing))]\n",
    "for i,j in missing_data:\n",
    "    data2[i,j]=np.nan\n",
    "\n",
    "# plot\n",
    "hyp.plot([data1, data2], linestyle=['-',':'], legend=['Original', 'PPCA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6834cfbe-2fa7-4ba5-9bf7-5b40b61b8226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ppca in d:\\conda\\envs\\py39\\lib\\site-packages (0.0.4)\n",
      "Requirement already satisfied: numpy in d:\\conda\\envs\\py39\\lib\\site-packages (from ppca) (1.24.4)\n",
      "Requirement already satisfied: scipy in d:\\conda\\envs\\py39\\lib\\site-packages (from ppca) (1.11.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ppca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95dbf500-a552-44c6-8b2e-d48bf21d03b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppca import PPCA\n",
    "ppca = PPCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f87c95aa-ee94-480d-ade5-c831f229f5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.6622113523210857\n",
      "1.7173829596225307\n",
      "1.6082670019509013\n",
      "0.40925052084155555\n",
      "0.19915006723391904\n",
      "0.11838535166400588\n",
      "0.07813037029146175\n",
      "0.05510118461279556\n",
      "0.04073232683772576\n",
      "0.03121244714005189\n",
      "0.024618566245064644\n",
      "0.01988980089605752\n",
      "0.01640089306062298\n",
      "0.013764276285280097\n",
      "0.011729989022336795\n",
      "0.010131530100216501\n",
      "0.008854952377665981\n",
      "0.007820442002301586\n",
      "0.006970944534084644\n",
      "0.00626493093693381\n",
      "0.005671678416352188\n",
      "0.005168122885478077\n",
      "0.004736716836989174\n",
      "0.004363946255991014\n",
      "0.004039286555737487\n",
      "0.0037544554503028404\n",
      "0.003502873242171134\n",
      "0.0032792656031614964\n",
      "0.0030793692526183136\n",
      "0.0028997112182822526\n",
      "0.0027374411775569207\n",
      "0.0025902033648075307\n",
      "0.002456037881657158\n",
      "0.0023333039447548387\n",
      "0.002220619413233038\n",
      "0.0021168133652631482\n",
      "0.0020208884039081276\n",
      "0.0019319902251453414\n",
      "0.0018493835795976299\n",
      "0.0017724326235384336\n",
      "0.0017005846012760895\n",
      "0.0016333572339986002\n",
      "0.00157032755732045\n",
      "0.0015111234580702515\n",
      "0.0014554159314343895\n",
      "0.001402913226370206\n",
      "0.0013533556651301737\n",
      "0.0013065111802601503\n",
      "0.0012621718348622135\n",
      "0.001220150732838432\n",
      "0.0011802792179318455\n",
      "0.0011424049578578899\n",
      "0.0011063896561509523\n",
      "0.0010721076789776962\n",
      "0.00103944451026039\n",
      "0.0010082954866390548\n",
      "0.0009785649257325613\n",
      "0.0009501649443004467\n",
      "0.0009230146975245379\n",
      "0.0008970398684915448\n",
      "0.0008721717117596839\n",
      "0.0008483467772697484\n",
      "0.0008255063080955249\n",
      "0.000803595843966054\n",
      "0.0007825646708223832\n",
      "0.0007623657659536764\n",
      "0.0007429552689268704\n",
      "0.0007242922481620617\n",
      "0.000706338457550082\n",
      "0.000689058213634075\n",
      "0.0006724181260926443\n",
      "0.0006563867998106154\n",
      "0.0006409348755975763\n",
      "0.0006260347518685094\n",
      "0.000611660460348995\n",
      "0.0005977875615137407\n",
      "0.0005843932209823599\n",
      "0.0005714555592524206\n",
      "0.0005589542948081228\n",
      "0.0005468701332675963\n",
      "0.0005351847092782691\n",
      "0.0005238809392655863\n",
      "0.0005129424850154329\n",
      "0.0005023538856399057\n",
      "0.0004921005233795572\n",
      "0.0004821685052229796\n",
      "0.0004725446899838115\n",
      "0.00046321659307690943\n",
      "0.00045417238278266936\n",
      "0.00044540076515309224\n",
      "0.0004368910018806016\n",
      "0.0004286328712950205\n",
      "0.00042061673643356556\n",
      "0.00041283332372121784\n",
      "0.00040527377832710876\n",
      "0.00039792967334095763\n",
      "0.0003907931631226269\n",
      "0.00038385638137339306\n",
      "0.0003771122102527169\n",
      "0.00037055361831717093\n",
      "0.0003641738918285231\n",
      "0.0003579667004487419\n",
      "0.00035192594504485086\n",
      "0.0003460457987578547\n",
      "0.0003403207258545482\n",
      "0.0003347453786795018\n",
      "0.00032931456521567704\n",
      "0.00032402345427118284\n",
      "0.0003188673513265794\n",
      "0.0003138417006531835\n",
      "0.0003089422241857598\n",
      "0.00030416479599670865\n",
      "0.00029950540950718363\n",
      "0.00029496022111152875\n",
      "0.000290525654867535\n",
      "0.0002861979643067336\n",
      "0.00028197402975171926\n",
      "0.00027785051430129215\n",
      "0.0002738241594228974\n",
      "0.0002698921419301836\n",
      "0.0002660514144303683\n",
      "0.0002622992110172806\n",
      "0.00025863292088534884\n",
      "0.0002550499058655564\n",
      "0.00025154765597457107\n",
      "0.0002481238135239039\n",
      "0.00024477605170747374\n",
      "0.00024150213896234618\n",
      "0.0002382999544241482\n",
      "0.00023516731021011594\n",
      "0.00023210232749515392\n",
      "0.00022910304329015752\n",
      "0.0002261675281403086\n",
      "0.0002232941603661498\n",
      "0.0002204809399017904\n",
      "0.00021772642581607826\n",
      "0.0002150288897275754\n",
      "0.00021238670335033127\n",
      "0.00020979849909852355\n",
      "0.00020726271989013334\n",
      "0.00020477794964279816\n",
      "0.00020234282101050383\n",
      "0.00019995607832101747\n",
      "0.00019761633247905586\n",
      "0.00019532245378983681\n",
      "0.00019307316414551856\n",
      "0.0001908672973263137\n",
      "0.00018870373926960227\n",
      "0.0001865814949002509\n",
      "0.0001844993964033037\n",
      "0.00018245643292980063\n",
      "0.00018045169338631872\n",
      "0.00017848413383703132\n",
      "0.00017655277811789993\n",
      "0.0001746568735436771\n",
      "0.00017279539887637974\n",
      "0.00017096765668433989\n",
      "0.00016917266021310162\n",
      "0.00016740973674833626\n",
      "0.0001656780823056092\n",
      "0.00016397685109126314\n",
      "0.00016230540367190294\n",
      "0.00016066303828310424\n",
      "0.0001590490492078267\n",
      "0.00015746273909811315\n",
      "0.00015590347347260725\n",
      "0.00015437068870882698\n",
      "0.00015286363856015228\n",
      "0.00015138185929863823\n",
      "0.00014992463551699586\n",
      "0.00014849153183904562\n",
      "0.00014708187694756347\n",
      "0.00014569532242258987\n",
      "0.00014433116937850876\n",
      "0.00014298905995513422\n",
      "0.00014166826957651146\n",
      "0.00014036867057432012\n",
      "0.00013908958608510602\n",
      "0.00013783045345472367\n",
      "0.00013659103963314934\n",
      "0.0001353708744369353\n",
      "0.000134169546095686\n",
      "0.00013298659108351707\n",
      "0.0001318216483368051\n",
      "0.00013067431184099476\n",
      "0.00012954422741073834\n",
      "0.00012843105428217427\n",
      "0.00012733439992440587\n",
      "0.00012625399591059683\n",
      "0.00012518936440031325\n",
      "0.00012414030300145207\n",
      "0.0001231064244309188\n",
      "0.0001220874750968992\n",
      "0.00012108305455038604\n",
      "0.00012009297041504219\n",
      "0.0001191168971588219\n",
      "0.00011815452379448921\n",
      "0.00011720562753136932\n",
      "0.00011626990644986179\n",
      "0.00011534707325555615\n",
      "0.00011443693878843142\n",
      "0.0001135392208679864\n",
      "0.00011265365764878688\n",
      "0.00011178008011136775\n",
      "0.00011091821063891061\n",
      "0.00011006778322708577\n",
      "0.00010922865619389377\n",
      "0.00010840051216431945\n",
      "0.000107583229137731\n",
      "0.00010677663159786022\n",
      "0.0001059804266623221\n",
      "0.0001051944653926995\n",
      "0.000104418533448003\n",
      "0.00010365253833444221\n",
      "0.00010289609040281533\n",
      "0.00010214928292917591\n",
      "0.0001014117544322346\n",
      "0.00010068338090407636\n",
      "9.99639439820399e-05\n"
     ]
    }
   ],
   "source": [
    "abs = ppca.fit(data=cdata.X, d=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bd03cce-c165-401f-b69f-af9cad71bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9bfce-d77e-4324-b9c1-337e73b29eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
